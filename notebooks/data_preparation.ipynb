{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rhSHb9c1rOb"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_fKlpl22ELw"
   },
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.2.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.2.0/en_core_web_lg-3.2.0-py3-none-any.whl (777.4 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m777.4/777.4 MB\u001B[0m \u001B[31m1.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:02\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from en-core-web-lg==3.2.0) (3.2.2)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.6)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.22.2)\r\n",
      "Requirement already satisfied: setuptools in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (58.1.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (4.62.3)\r\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (8.0.13)\r\n",
      "Requirement already satisfied: jinja2 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.3)\r\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.4.0)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.6)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.0.1)\r\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.6.1)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.3.0)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.8)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.4.2)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.6)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.9.0)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.0.6)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.8.2)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.7.5)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.27.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (21.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.7)\r\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (5.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (4.0.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.26.8)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.11)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (8.0.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.1)\r\n",
      "Installing collected packages: en-core-web-lg\r\n",
      "Successfully installed en-core-web-lg-3.2.0\r\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 23.0 is available.\r\n",
      "You should consider upgrading via the '/Users/alexandredias/.pyenv/versions/3.9.13/envs/sdg-classifier/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_lg')\r\n"
     ]
    }
   ],
   "source": [
    "# It takes 2 minutes to run this cell \n",
    "!python -m spacy download en_core_web_lg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "executionInfo": {
     "elapsed": 11443,
     "status": "error",
     "timestamp": 1658438954279,
     "user": {
      "displayName": "Ariel Alsina",
      "userId": "06765261206681375168"
     },
     "user_tz": 180
    },
    "id": "vR-j7MCe10vl",
    "outputId": "719e705d-7d85-434a-94c7-a04fd5ad701d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "# import gdown\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# For advanced NLP Processing\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import en_core_web_lg\n",
    "import re\n",
    "\n",
    "from unicodedata import normalize, combining\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1658298210736,
     "user": {
      "displayName": "Ariel Alsina",
      "userId": "06765261206681375168"
     },
     "user_tz": 180
    },
    "id": "RrVec5afRoEO",
    "outputId": "233993e3-cd11-457d-b8d1-9a5f7a508e39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MUy2jVmr7QkS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alexandredias/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alexandredias/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "os.chdir(parent_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mC0FGoEb2GcA"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmbq-PVB79cN"
   },
   "source": [
    "`load_data` ler todos os arquivos individuais de todas as SDGs e os concatena, retornando um dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thtvp_D25yqH"
   },
   "outputs": [],
   "source": [
    "# Download CSV files\n",
    "# url = \"https://drive.google.com/drive/folders/1-cwm0B2kVXpbTT4qbLfEJlyRFMtD1gDm\"\n",
    "# gdown.download_folder(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fqo8ztwq2H1q"
   },
   "outputs": [],
   "source": [
    "# Load data of all SDGs from every source file \n",
    "def load_data():\n",
    "    files = glob.glob(\"./data/csv/sdg/*.csv\")\n",
    "    print(files)\n",
    "    datasets = []\n",
    "    for file in files:\n",
    "        datasets.append(pd.read_csv(file, sep=\"\\t\"))\n",
    "        \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        targets = mlb.fit_transform(dataset[\"Sustainable Development Goals (2021)\"].\\\n",
    "                                    str.replace(\" \", \"\").\\\n",
    "                                    str.split(\"|\"))\n",
    "        targets_dataframe = pd.DataFrame(targets, columns=mlb.classes_, dtype=np.float32)\n",
    "\n",
    "        datasets[i] = pd.concat([datasets[i], targets_dataframe], axis=1)\n",
    "        datasets[i] = datasets[i].drop(columns=[\"Sustainable Development Goals (2021)\"])\n",
    "        \n",
    "    samples = []\n",
    "    for dataset in datasets:\n",
    "        samples.append(dataset)\n",
    "        \n",
    "    data = pd.concat(samples)\n",
    "    data = data.rename(columns={\"Title\": \"text\"})\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jB8P45i2NIj"
   },
   "source": [
    "**Balanceamento de dataset para tarefa de classifição multilabel**\n",
    "\n",
    "A ocorrência dos rótulos (SDGs) é severamente desbalanceada. Considere, por exemplo, a quantidade de ocorrências tirada do dataset geral para cada SDG:\n",
    "\n",
    "|SDG1|SDG2|SDG3|SDG4|SDG5|SDG6|SDG7|SDG8|SDG9|SDG10|SDG11|SDG12|SDG13|SDG14|SDG15|SDG16|\n",
    "|----|----|----|----|----|----|----|----|----|-----|-----|-----|-----|-----|-----|-----|\n",
    "|83470|181140|249191|151681|120897|238204|343391|278841|351881|222361|297820|256995|264076|135624|167590|159497|\n",
    "\n",
    "Note que algumas SDGs têm mais de 300k ocorrências, enquanto algumas outras tem menos de 200k ocorrências. Problemas de classificação com datasets desbalanceadas são ainda mais desafiadores. Para nossa conveniências, e levando em conta que temos muitos dados, podemos tentar balancear esse conjunto de dados para só então iniciar o processo de treinamento de fato.\n",
    "\n",
    "Pensando nisso, foi desenvolvido um algoritmo para balancear o dataset. A ideia geral é realizada em 5 passos:\n",
    "\n",
    "1. Identificar qual classe tem a menor ocorrência no dataset geral (`data`). Vamos chamar essa classe de `base_class`.\n",
    "2. Coletar do dataset geral todas as amostras com ocorrência da `base_class`. Criar novo dataset com essas amostras, chamado `keeper`.\n",
    "3. Após a remover todas as ocorrências da `base_class` do `data`: Identificar quais classes restaram tal que a sua quantidade de ocorrências sejam menores que a quantidade de ocorrências da `base_class`. Vamos chamar o conjunto das classes identificadas de `compromised`. O complemento da classe compromised, é o conjunto de classes com ocorrências maiores que a `base_class`, vamos chamá-los de `intransigent`.\n",
    "\n",
    "4. Para cada classe `i` do conjunto `compromised`:\n",
    "- Usando o dataset `data`, colete todas as amostras com ocorrências da classe `i` de `compromised` tais que nessas amostras não hajam ocorrências das classes de `intransigent`. O conjunto dessas amostras coletadas será chamado de `concession`.\n",
    "- Adicione `n_samples` amostras do conjunto `concession` ao conjunto `keeper`, onde N = (_Número de ocorrências da `base_class`_ - _Número de ocorrências da classe `i` no conjunto_ `compromised`)\n",
    "\n",
    "5. Repita o item 3 e 4 até que _o conjunto `compromised` pare de mudar ou fique vazio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qz3E1oYB2PS2"
   },
   "outputs": [],
   "source": [
    "def balance_dataframe(data):\n",
    "    # step 1\n",
    "    counts = data.sum(axis=0)\n",
    "    base_class_count, base_class_idx = np.min(counts), np.argmin(counts)\n",
    "    \n",
    "    # step 2\n",
    "    # initiliaze keeper dataset\n",
    "    keeper = data[data.iloc[:, base_class_idx] == 1]\n",
    "\n",
    "    # remove records added to the keeper dataset\n",
    "    data = data[data.iloc[:, base_class_idx] == 0]\n",
    "    \n",
    "    # step 3\n",
    "    # identify classes from keeper that have more instances than base_class_count\n",
    "    intransigent = np.sum(keeper, axis=0) >= base_class_count\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        compromised = np.sum(keeper, axis=0) < base_class_count\n",
    "        \n",
    "        # step 5.1: check if compromised stopped changing\n",
    "        if np.all(intransigent == compromised):\n",
    "            return keeper\n",
    "        \n",
    "        # step 4\n",
    "        # step 4.1\n",
    "        intransigent_classes_idx = np.concatenate(np.argwhere(np.array(~compromised)))\n",
    "        \n",
    "        balance_mask = np.full((data.shape[0],), True)\n",
    "        for j in intransigent_classes_idx:\n",
    "            balance_mask = balance_mask & (data.iloc[:, j] == 0)\n",
    "        \n",
    "        concession = data.loc[balance_mask, :]\n",
    "        data = data.loc[balance_mask, :]\n",
    "        \n",
    "        # step 5.1: check if concession only have 0's (is empty)\n",
    "        if sum(np.sum(concession)) == 0:\n",
    "            return keeper\n",
    "        \n",
    "        # step 4.2\n",
    "        compromised_classes_idx = np.array(compromised).nonzero()[0]\n",
    "\n",
    "        if len(compromised_classes_idx) > 0:\n",
    "            compromised_class = np.array(compromised).nonzero()[0][0]\n",
    "            \n",
    "            n_sampleable = np.sum(concession.iloc[:, compromised_class])\n",
    "\n",
    "            n_samples = base_class_count - np.sum(keeper, axis=0)[compromised_classes_idx[0]]\n",
    "            \n",
    "            if n_samples > n_sampleable:\n",
    "                n_samples = n_sampleable\n",
    "                \n",
    "            data = data.loc[concession.iloc[:, compromised_class] == 0, :]\n",
    "            concession = concession[concession.iloc[:, compromised_class] == 1][:n_samples]\n",
    "        \n",
    "        # update keeper and intransigent sets for the next iteration\n",
    "        keeper = pd.concat([keeper, concession])\n",
    "        intransigent = compromised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "enEF-79q4qMM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/csv/sdg/sdg_02.csv', './data/csv/sdg/sdg_16.csv', './data/csv/sdg/sdg_03.csv', './data/csv/sdg/sdg_15.csv', './data/csv/sdg/sdg_01.csv', './data/csv/sdg/sdg_14.csv', './data/csv/sdg/sdg_10.csv', './data/csv/sdg/sdg_04.csv', './data/csv/sdg/sdg_05.csv', './data/csv/sdg/sdg_11.csv', './data/csv/sdg/sdg_07.csv', './data/csv/sdg/sdg_13.csv', './data/csv/sdg/sdg_12.csv', './data/csv/sdg/sdg_06.csv', './data/csv/sdg/sdg_08.csv', './data/csv/sdg/sdg_09.csv']\n"
     ]
    }
   ],
   "source": [
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KopbQgBZ2StR"
   },
   "outputs": [],
   "source": [
    "# Remove duplicate titles\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Remove duplicate titles with different targets\n",
    "counts = data[\"text\"].value_counts()\n",
    "titles, counts = list(counts.index), list(counts)\n",
    "\n",
    "for title, count in zip(titles, counts):\n",
    "    if count > 1:\n",
    "        data = data.loc[data[\"text\"] != title, :]\n",
    "    \n",
    "    # since the list of counts is ordered, if it gets to count == 1, then we can break the loop\n",
    "    if count == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OqTtW1UX4tkN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xl/1201d_y17px6m0yfh4zzd24m0000gq/T/ipykernel_13472/816826153.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  titles[\"index\"] = titles.index\n"
     ]
    }
   ],
   "source": [
    "titles = data[[\"text\"]]\n",
    "unbalanced_targets = data.iloc[:, 1:]\n",
    "\n",
    "# get balanced dataset based on SDGs occurrences\n",
    "unbalanced_targets = unbalanced_targets.astype(np.int64)\n",
    "targets = balance_dataframe(unbalanced_targets).astype(np.float32)\n",
    "\n",
    "# Set indices as a column for further dataframe merging\n",
    "targets[\"index\"] = targets.index\n",
    "titles[\"index\"] = titles.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5QknI1Lx2cTB"
   },
   "outputs": [],
   "source": [
    "# Join targets and titles dataframe\n",
    "data = targets.merge(titles, how=\"left\", on=\"index\")\n",
    "data = data.set_index(\"index\")\n",
    "\n",
    "columns = [\"text\", \"SDG1\", \"SDG2\", \"SDG3\", \"SDG4\", \"SDG5\", \"SDG6\", \"SDG7\", \"SDG8\", \"SDG9\", \n",
    "           \"SDG10\", \"SDG11\", \"SDG12\", \"SDG13\", \"SDG14\", \"SDG15\", \"SDG16\"]\n",
    "data = data[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhaLwSK32dXV"
   },
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAOAPatb2enO"
   },
   "source": [
    "### Split train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3VMFM7P2jdO"
   },
   "outputs": [],
   "source": [
    "sdg_columns = list(data.columns[1:])\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(data[\"text\"]), np.array(data[sdg_columns]),\n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvZxIvX52lo3"
   },
   "outputs": [],
   "source": [
    "train_size = round(0.9 * X_train.shape[0])\n",
    "\n",
    "X_valid, y_valid = X_train[train_size:], y_train[train_size:]\n",
    "X_train, y_train = X_train[:train_size], y_train[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650573263238,
     "user": {
      "displayName": "ALEXANDRE DIAS",
      "userId": "09695842364651727728"
     },
     "user_tz": 180
    },
    "id": "zXYBQpqK2nHC",
    "outputId": "54b7cfdb-7e6e-4087-929f-cf0e0a3453eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: \t454432 records.\n",
      "validation set: 50492 records.\n",
      "test set: \t126232 records.\n"
     ]
    }
   ],
   "source": [
    "print(\"train set: \\t{} records.\".format(X_train.shape[0]))\n",
    "print(\"validation set: {} records.\".format(X_valid.shape[0]))\n",
    "print(\"test set: \\t{} records.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRhjWODU2oJa"
   },
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZKELkwf2qGJ"
   },
   "source": [
    "O pré-processamento textual é dividido em duas etapas:\n",
    "\n",
    "1. Na primeira etapa, são realizadas operações avançadas de PLN que não podem ser realizadas com funções nativas do TensorFlow, tais como: \n",
    "- Remoção de acentos, acentuaçao e caracteres especiais;\n",
    "- Remoção de stopwords;\n",
    "- Lematização ou Stemming;\n",
    "- Filtragem.\n",
    "\n",
    "2. A segunda etapa consiste em converter os datasets de `numpy` para o formato padrão do TensorFlow `tf.data.Dataset`. \n",
    "\n",
    "Ainda antes de alimentar o modelo com este dataset, precisamos vetorizar as sequências de texto. Esta etapa é realizada apenas no notebook de treinamento do modelo. Ela consiste em passar os dados por uma camada `TextVectorization` nativa do TensorFlow. Essa camada realiza:\n",
    "- Padding das sequências de texto;\n",
    "- codificação/vetorização de palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrKxDd982smn"
   },
   "source": [
    "#### Advanced NLP Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKANKP5f2v2F"
   },
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    nltk_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    spacy_en = en_core_web_lg.load()\n",
    "    spacy_stopwords = spacy_en.Defaults.stop_words\n",
    "    \n",
    "    stopwords = list(set(spacy_stopwords).union(set(nltk_stopwords)))\n",
    " \n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPYxsgY_2z4W"
   },
   "outputs": [],
   "source": [
    "def advanced_preprocess(X, y, truncation=\"lemma\"):\n",
    "    # Convert text to lowercase\n",
    "    Z = [text.lower() for text in X] \n",
    "\n",
    "    # Remove special characters\n",
    "    special_char_reg_ex=\"!@#$%^&*()[]{};:,./<>?\\|`~-=_+123456789\"\n",
    "    Z = [text.translate({ord(char): \" \" for char in special_char_reg_ex}) for text in Z]\n",
    "\n",
    "    # Remove numbers\n",
    "    Z = [re.sub(r\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$|\\d+\\)\", ' ', text) for text in Z]\n",
    "\n",
    "    # Remove double spaces\n",
    "    Z = [re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text) for text in Z]\n",
    "\n",
    "    # Remove accents\n",
    "    Z = [\"\".join([char for char in normalize(\"NFKD\", text) if not combining(char)]) for text in Z]\n",
    "\n",
    "    # Tokenize text\n",
    "    Z = [word_tokenize(text) for text in Z]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stopwords = get_stopwords()\n",
    "    Z = [list((word for word in tokens if ((word not in stopwords) and (len(word) > 1)))) for tokens in Z]\n",
    "    \n",
    "    # Lemmatizing\n",
    "    if truncation == \"lemma\":\n",
    "        # Concatenate tokens\n",
    "        Z = [\" \".join(tokens) for tokens in Z]\n",
    "\n",
    "        # Lemmatize sentences\n",
    "        nlp = en_core_web_lg.load()\n",
    "        lemmatize = lambda sentence: \" \".join([token.lemma_ for token in nlp(sentence)])\n",
    "        Z = [lemmatize(text) for text in tqdm(Z)]\n",
    "    \n",
    "    # Stemming\n",
    "    if truncation == \"stem\":\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        Z = [\" \".join([stemmer.stem(token) for token in tokens]) for tokens in Z]\n",
    "        \n",
    "    if truncation is None:\n",
    "        Z = [\" \".join(tokens) for tokens in Z]\n",
    "\n",
    "    # Convert back to np.array\n",
    "    Z = np.array(Z)\n",
    "    \n",
    "    # Discard empty sentences \n",
    "    non_empty_sentences = Z != \"\" \n",
    "    y = y[non_empty_sentences]\n",
    "    Z = Z[non_empty_sentences]\n",
    "    \n",
    "    return Z, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5070508,
     "status": "ok",
     "timestamp": 1650578333740,
     "user": {
      "displayName": "ALEXANDRE DIAS",
      "userId": "09695842364651727728"
     },
     "user_tz": 180
    },
    "id": "11xzv5Wx21hP",
    "outputId": "06bc4ff8-7151-4130-bf01-8b848c4392ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454432/454432 [1:22:04<00:00, 92.28it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = advanced_preprocess(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 587838,
     "status": "ok",
     "timestamp": 1650578921558,
     "user": {
      "displayName": "ALEXANDRE DIAS",
      "userId": "09695842364651727728"
     },
     "user_tz": 180
    },
    "id": "K7oW6jTs22h_",
    "outputId": "609856a8-c588-4f05-d882-9b78f3f5736d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50492/50492 [09:10<00:00, 91.76it/s]\n"
     ]
    }
   ],
   "source": [
    "X_valid, y_valid = advanced_preprocess(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1416291,
     "status": "ok",
     "timestamp": 1650580337831,
     "user": {
      "displayName": "ALEXANDRE DIAS",
      "userId": "09695842364651727728"
     },
     "user_tz": 180
    },
    "id": "8ZhpE9_523vt",
    "outputId": "90825b16-676e-4b97-c4f9-c4bb11779327"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126232/126232 [22:40<00:00, 92.79it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = advanced_preprocess(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLLyPqBe25Pr"
   },
   "source": [
    "#### Build TF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdMFbQ5D3WQ_"
   },
   "outputs": [],
   "source": [
    "def create_dataset(X, y):\n",
    "    return tf.data.Dataset.from_tensor_slices((tf.constant(X), tf.constant(y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BH8cA-g3YIa"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# build train set\n",
    "train_set = create_dataset(X_train, y_train).\\\n",
    "    shuffle(X_train.shape[0], seed=42).batch(batch_size).prefetch(1)\n",
    "\n",
    "# build validation set\n",
    "valid_set = create_dataset(X_valid, y_valid).batch(batch_size).prefetch(1)\n",
    "\n",
    "# build test set\n",
    "test_set = create_dataset(X_test, y_test).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlrvAwhK7Mju"
   },
   "outputs": [],
   "source": [
    "!mkdir datasets_sdg_tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMREmStj3Zct"
   },
   "outputs": [],
   "source": [
    "# stores tf datasets\n",
    "tf.data.experimental.save(train_set, \"./datasets_sdg_tensorflow/train_set\")\n",
    "tf.data.experimental.save(valid_set, \"./datasets_sdg_tensorflow/valid_set\")\n",
    "tf.data.experimental.save(test_set, \"./datasets_sdg_tensorflow/test_set\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('3.9.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "da5931478216a435c18c3fc9f288d30c232ddb8d5d62ae2db51b5a7845391c34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
